# Installation

# galaxy2galaxy

Galaxy2galaxy is used for the dataset generation. The version in the repositories, is not compatible with this project. Luckily the compatible version can easily be installed by just installing the ShapeNetL1.yaml environment, that can be found in the environments directory:

1. Clone this repository

```bash
git clone https://github.com/dkonsoul/ShapeNetL1
```

2. Install ShapeNetL1 environment:

```
cd ShapeNetL1/environments
export PYTHONNOUSERSITE=1
conda env create -f ShapeNetL1.yaml
```

2. Activate the environment:

```bash
conda activate ShapeNetL1
```

3. Download the TFRecord files:

```bash
galsim_download_cosmos -s 23.5
```

4. Generate the training data
```bash
mkdir data_dir_optical
g2g-datagen --problem=attrs2img_cosmos_cfht2hst --data_dir=/home/data_dir_optical/
```
The data_dir_optical directory, will get populated with files that our main project will use to simulate galaxy images, during both the training and the evaluation process. You will need to specify where you created this in later stages.

# Generate the dataset file using scripts

To generate a file, so all ShapeNet, ShapeNetL1 and SUNet use the same dataset, we need to run some external scripts. Note that while ShapeNet and ShapeNetL1 can generate their dataset internally, if keeping the same dataset as input
for all of them is not needed, SUNet needs this to be run first since it expects the dataset to be ready externally. This script can be found inside the scripts directory of this project. You can run it like this:

```bash
python extract_training_dataset.py --data_dir=/home/data_dir_optical
```

The output file (saved_dataset.npz) must be moved inside your ShapeNetL1 directory, alongside the train script, which will be described below.

In order to generate the file structure and images that SUNet expects, we need to run one more script, which can be found in the same directory.

```bash
python export_images_for_SUNet.py
```

This script expects the saved_dataset.npz to be in the same directory. After running it, a new folder will be generated, named dataset. This will has to be moved inside SUNet directory.

# ShapeNet and ShapeNetL1

The script to train both SpaneNet and ShapeNetL1, is located inside ShapeNetL1 directory and is named train.py. Let's see the most important acceptable parameters it can accept:

```text
--use_gpu -> If this is set, the training will attempt to use the system's GPU. By default it will only use the CPU.
--model_dir=/home/models/ -> This parameter controlls where the model weights will be saved after training is complete.
--load_dataset_from_file -> If this is set, the script will load the dataset from a file in tha same directory, named saved_dataset.npz. This file can be generated using the scripts found in this repository.
--loss_type=L1 -> This can be set to either L2 (uses ShapeNet) or L1 (uses ShapeNetL1), by default it is set to L2.
--data_dir=/home/galaxy2galaxy/data_dir_optical -> This sets where the data generated by galaxy2galaxy are located. Used when --load_dataset_from_file is not set, to generate the dataset on the run. This is the path that we saved our dataset in the previous step, in this tutorial named data_dir_optical
--shape_constraint=multi -> This can be set to None, multi or single. None will run Tikhonet (not used in this research), multi will run ShapeNet with multiwindow (what is used here) and single will use single window (not used here either)
All these are described in the original ShapeNet paper.
--gamma=0.0078125 -> This is the weight of the Shape Constraint inside the loss function. By default it is set to 0.5, which is optimal for ShapeNet when using L2 norm. For L1 norm this has to be set to 0.0078125 for optimal results
```

Taking all these into account, to train ShapeNet we have to run this command (assuming we want the models to be saved in "/home/models":

```bash
python train.py --model_dir=/home/models/ --shape_constraint=multi --load_dataset_from_file
```
Of course, we need to make sure that the file saved_dataset.npz is in the same directory as our script.

To run ShapeNetL1

```bash
python train.py --model_dir=/home/models --shape_constraint=multi --load_dataset_from_file --loss_type=L1 --gamma=0.0078125
```

If we wish to generate the data on the run, without using the dataset file, which will use less memory and be faster, but will result on our two models using a different dataset to train, we can run them like this instead:

ShapeNet:

```bash
python train.py --model_dir=/home/models --data_dir=/home/data_dir_optical/ --shape_constraint=multi
```
ShapeNetL1:

```bash
python train.py --model_dir=/home/models --data_dir=/home/data_dir_optical/ --shape_constraint=multi --loss_type=L1 --gamma=0.0078125
```

# SUNet

The original SUNet was adjusted for this project to work with the dataset generated. To use it you will need to create a new environment, using SUNet_env.yaml, inside the environments directory:

```bash
export PYTHONNOUSERSITE=1
conda env create -f SUNet.yaml
```

Then git clone my fork to your desired directory:

```bash
git clone https://github.com/dkonsoul/SUNet
```

Then, copy over the dataset directory, which you generated after running the export_images_for_SUNet.py script above, to the root directory of SUNet.

Finally run SUNet training by simply running

```bash
python train.py
```

# Evaluation metrics

To run the evaluation metrics, we need to run some jupyter notebook scripts, located inside the evaluation directory. We first need to make sure we are on the correct conda environment, then run Jupyter Notebook

```bash
conda activate ShapeNetL1
jupyter-notebook --no-browser --ip=0.0.0.0 --NotebookApp.token='' --NotebookApp.password=''
```

Inside the evaluation directory, we will find 4 scripts, numbered in order that you need to run them:

```text
/
├── 1. extract_evaluation_dataset.ipynb # Generates an evaluation dataset, which we need to feed to our models, before the evaluation
├── 2. extract_data_for_plots.ipynb # Extracts SUNet's results and runs inference for ShapeNet and ShapeNetL1 before the evaluation
├── 3. make_plots.ipynb # Creates the evaluation plots
└── 4. extract_images.ipynb # Creates the output image comparison
```


For every single one of these scripts, we need to make sure to update all the paths that can be found in the first section accordingly:

```text
lib_path -> Should point to the ShapeNetL1 directory, where the cadmos_lib is

data_path -> Should point to the directory we generated the dataset using galaxy2galaxy

model_dir -> Should point to where our ShapeNet and ShapeNetL1 models are located

path_SUNet -> Should point to SUNet.npy, which is generated by our first jupyter notebook (where applicable)
```

1) First we run the "1. extract_evaluation_dataset.ipynb" notebook. This will generate a file named "cfht_batch.pkl", inside our data_dir directory. This contains the test dataset and is needded from both the rest of the scripts and SUNet itself to generate it's own results.

Then we need to run SUNet against this evaluation dataset. In order to do that, we need to get inside our SUNet environment and use the "comparison_batch_to_npy" script that was specifically made for this purpose and can be found inside our modified SUNet git:

```text
conda activate SUNet
python comparison_batch_to_npy.py --weights './checkpoints/Denoising/models/model_bestPSNR.pth' --cfht_batch_path '/home/data_dir_optical/'
```
where --weights point to the weights generated by training SUNet and --cfht_batch_path is our data_dir that we just exported our evaluation dataset in the previous step. This script will export a file named SUNet.npy, in which our next jupyter notebook scripts will need to point to.

2) Next, we need to run "2. extract_data_for_plots.ipynb", which will take the generated output from SUNet and run ShapeNet and ShapeNetL1 as well and generate the final results that our next script will use to create the plots.

3) The next notebook we need is "3. make_plots.ipynb", which will generate our final plots. Again, as in the previous steps, you need to update the paths as neccesary.

4) Last but not least, we can run "4. extract_images.ipynb" to plot out an image grid, of images denoised by each model.
